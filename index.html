<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>&ON</title>
    <link rel="stylesheet" href="./style.css" />
  </head>
  <style>
    @font-face {
      font-family: "Asflat";
      src: url("font/Asflat.woff2") format("woff2");
    }

    body {
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      height: 100vh;
      font-family: "Asflat", sans-serif;
    }

    #handStatus {
      font-size: 60vw;
      text-align: center;
      margin: 0;
    }

    #notification {
      font-size: 1vw;
      color: black;
      text-align: center;
      margin: 0;
      display: none;
      font-family: monospace; /* Set font family to monospace */
      text-transform: uppercase; /* Set text to uppercase */
    }
  </style>
  <script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
  <script
    src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"
    crossorigin="anonymous"
  ></script>
  <script
    src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"
    crossorigin="anonymous"
  ></script>
  <script src="https://unpkg.com/tone@14.8.49/build/Tone.js"></script>

  <body>
    <div id="liveView" class="videoView">
      <button id="webcamButton" class="mdc-button mdc-button--raised">
        <span class="mdc-button__ripple"></span>
        <span class="mdc-button__label">ENABLE WEBCAM</span>
      </button>
      <p id="handStatus"></p>
      <p id="notification">
        Please place your hand in front of the camera to activate the
        synthesizer
      </p>
      <div style="position: relative">
        <!-- Video element is hidden by setting its display to none -->
        <video id="webcam" style="display: none" autoplay playsinline></video>
        <canvas
          class="output_canvas"
          id="output_canvas"
          style="display: none; position: absolute; left: 0px; top: 0px"
        ></canvas>
      </div>
    </div>

    <!-- partial -->
    <script type="module">
      
      import {
        HandLandmarker,
        FilesetResolver,
      } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

      const demosSection = document.getElementById("demos");
      let lastHandDetectedTime = 0;
      let handLandmarker = undefined;
      let runningMode = "IMAGE";
      let enableWebcamButton;
      let webcamRunning = false;
      let results = undefined;

      // Tone.js Audio Players
      const onPlayer = new Tone.GrainPlayer({
        url: "sound/on.mp3",
        loop: true,
      }).toDestination();

      const andPlayer = new Tone.GrainPlayer({
        url: "sound/and.mp3",
        loop: true,
        grainSize: 0.1,
      }).toDestination();

      // Before we can use HandLandmarker class we must wait for it to finish
      // loading. Machine Learning models can be large and take a moment to
      // get everything needed to run.
      const createHandLandmarker = async () => {
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
        );
        handLandmarker = await HandLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task`,
            delegate: "GPU",
          },
          runningMode: runningMode,
          numHands: 1,
        });
        // demosSection.classList.remove("invisible");
      };

      createHandLandmarker();

      const video = document.getElementById("webcam");
      const canvasElement = document.getElementById("output_canvas");
      const canvasCtx = canvasElement.getContext("2d");

      // Check if webcam access is supported.
      const hasGetUserMedia = () => {
        var _a;
        return !!((_a = navigator.mediaDevices) === null || _a === void 0
          ? void 0
          : _a.getUserMedia);
      };

      // If webcam supported, add event listener to button for when user
      // wants to activate it.
      if (hasGetUserMedia()) {
        enableWebcamButton = document.getElementById("webcamButton");
        enableWebcamButton.addEventListener("click", enableCam);
      } else {
        console.warn("getUserMedia() is not supported by your browser");
      }

      // Enable the live webcam view and start detection.
      // Enable the live webcam view and start detection.
      function enableCam(event) {
        // Start or resume the Tone audio context
        Tone.start()
          .then(() => {
            console.log("AudioContext started");
            // AudioContext has been started or resumed, continue with your application logic
          })
          .catch((err) => console.error("AudioContext failed to start:", err));

        if (!handLandmarker) {
          console.log("Wait! objectDetector not loaded yet.");
          return;
        }

        if (webcamRunning === true) {
          webcamRunning = false;
          enableWebcamButton.innerText = "ENABLE PREDICTIONS";
        } else {
          webcamRunning = true;
          enableWebcamButton.innerText = "DISABLE PREDICTIONS";
        }

        // Remove the enable webcam button from the DOM
        enableWebcamButton.remove();

        // getUsermedia parameters.
        const constraints = {
          video: true,
        };

        // Activate the webcam stream.
        navigator.mediaDevices.getUserMedia(constraints).then((stream) => {
          video.srcObject = stream;
          video.addEventListener("loadeddata", predictWebcam);
        });
      }

      let lastVideoTime = -1;
      async function predictWebcam() {
  // Ensure the canvas dimensions match the video dimensions.
  canvasElement.style.width = video.videoWidth + "px";
  canvasElement.style.height = video.videoHeight + "px";
  canvasElement.width = video.videoWidth;
  canvasElement.height = video.videoHeight;

  // Switch to 'VIDEO' mode if necessary.
  if (runningMode === "IMAGE") {
    runningMode = "VIDEO";
    await handLandmarker.setOptions({ runningMode: "VIDEO" });
  }

  if (lastVideoTime !== video.currentTime) {
    lastVideoTime = video.currentTime;
    results = await handLandmarker.detectForVideo(
      video,
      performance.now()
    ); // Await detection.
  }

  // Clear the canvas for the new frame.
  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

  // If hand landmarks are detected...
  if (results && results.landmarks && results.landmarks.length > 0) {
    const landmarks = results.landmarks[0]; // Assuming there's only one hand detected.
    const wristLandmark = landmarks[0]; // Assuming the first landmark is the wrist.

    // Calculate font width based on the wrist's x-coordinate.
    // Assuming wristLandmark.x is normalized between 0 and 1.
    let fontWidthValue;
    if (wristLandmark.x <= 0.5) {
      // Map 0 to 0.5 to font width from 100 to 50
      fontWidthValue = 100 - wristLandmark.x * 100; // 100 - (0 to 0.5) * 100
    } else {
      // Map 0.5 to 1 to font width from 50 to 100
      fontWidthValue = 50 + (wristLandmark.x - 0.5) * 100; // 50 + (0 to 0.5) * 100
    }

    // Calculate font weight based on the wrist's y-coordinate.
    // Assuming wristLandmark.y is normalized between 0 and 1.
    const fontWeightValue = 300 + wristLandmark.y * 400; // 300 + (0 to 1) * 400

    // Update the body's font variation settings directly.
    document.body.style.fontVariationSettings = `'wdth' ${fontWidthValue}, 'wght' ${fontWeightValue}`;

    // Map x-position to playback rate
    let playbackRate = mapValue(fontWidthValue, 50, 100, 10, 0.001);
    // Clamp the playbackRate to ensure it does not go below 0.001 and does not exceed 10
    playbackRate = Math.max(0.001, playbackRate); // Ensure it's not less than 0.001
    playbackRate = Math.min(playbackRate, 10); // Ensure it's not more than 10

    // Map y-position to grainSize value
    const grainSizeValue = mapValue(wristLandmark.y, 0, 1, 1, 0.05);

    // Clamp the grainSize to ensure it's within the valid range
    const clampedGrainSize = Math.max(0.05, Math.min(grainSizeValue, 1)); // Assuming 0.05 is the min and 1 is the max valid grainSize

    // Set the grainSize of the audio players
    onPlayer.grainSize = clampedGrainSize;
    andPlayer.grainSize = clampedGrainSize;

    // Assign the clamped playback rate to your Tone.GrainPlayer instances
    onPlayer.playbackRate = playbackRate;
    andPlayer.playbackRate = playbackRate;

    // Update the last detected time
    lastHandDetectedTime = performance.now();

    // Determine which side of the screen the hand is on
    const handStatusElement = document.getElementById("handStatus");
    if (wristLandmark.x < 0.5) {
      handStatusElement.textContent = "on";
      document.body.style.backgroundColor = "black";
      document.getElementById("notification").style.color = "white";
      document.getElementById("handStatus").style.color = "white";
      if (!onPlayer.state.startsWith("started")) {
        onPlayer.start();
      }
      andPlayer.stop();
    } else {
      document.body.style.backgroundColor = "white";
      document.getElementById("handStatus").style.color = "black";
      document.getElementById("notification").style.color = "black";

      handStatusElement.textContent = "and";
      if (!andPlayer.state.startsWith("started")) {
        andPlayer.start();
      }
      onPlayer.stop();
    }

    // Visualization: draw the detected landmarks and connections on the canvas.
    drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
      color: "#00FF00",
      lineWidth: 5,
    });
    drawLandmarks(canvasCtx, landmarks, {
      color: "#FF0000",
      lineWidth: 2,
    });

    // Hide the notification if hands are detected
    document.getElementById("notification").style.display = "none";
    document.getElementById("handStatus").style.display = "block";
  } else {
    // Check if 2 seconds have passed since the last hand detection
    const currentTime = performance.now();
    const elapsedTime = (currentTime - lastHandDetectedTime) / 1000; // Convert milliseconds to seconds

    if (elapsedTime >= 2) {
      // Show the notification if no hands are detected and 2 seconds have passed
      document.getElementById("notification").style.display = "block";
      document.getElementById("handStatus").style.display = "none";
    } else {
      // Hide the notification if less than 2 seconds have passed since the last hand detection
      document.getElementById("notification").style.display = "none";
      document.getElementById("handStatus").style.display = "block";
    }
  }

  canvasCtx.restore();

  // Continue predicting if the webcam is still running.
  if (webcamRunning === true) {
    window.requestAnimationFrame(predictWebcam);
  }
}

      function mapValue(value, minInput, maxInput, minOutput, maxOutput) {
        return (
          minOutput +
          (maxOutput - minOutput) * ((value - minInput) / (maxInput - minInput))
        );
      }

      document.addEventListener("DOMContentLoaded", function () {});
    </script>
  </body>
</html>
